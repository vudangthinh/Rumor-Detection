{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.sciencedirect.com/science/article/pii/S0306457318307957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import torch\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "import networkx as nx\n",
    "import random\n",
    "import gc\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "gc.collect()\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'dataset': 'Twitter', #Twitter/PHEME     \n",
    "    'data_dir': '/data/rumor_detection/data/rumor_acl/rumor_detection_acl2017/twitter_all/', #Twitter\n",
    "#     'data_dir': '/data/rumor_detection/data/pheme/pheme_v2_extend/pheme_twitter/', #PHEME\n",
    "    'tweet_content_file': 'tweet_contents_merge_fix.txt',\n",
    "    'tree_dir': 'tree',\n",
    "    'label_file': 'label.txt',\n",
    "    'user_info_file': 'user_info.txt',\n",
    "    'w2v': 'twitter_preprocess_4_w2c_400.txt',\n",
    "\n",
    "    'max_graph_size': 50,\n",
    "    'n_splits': 5,\n",
    "    'seed': 1234,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(args['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format(args['data_dir'] + args['w2v'], binary=False)\n",
    "embed_dim = word_vectors.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_processor():\n",
    "    text_processor = TextPreProcessor(\n",
    "            normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "                       'time', 'date', 'number'],\n",
    "            fix_html=True,\n",
    "            segmenter=\"twitter\",\n",
    "            corrector=\"twitter\",\n",
    "\n",
    "            unpack_hashtags=True,\n",
    "            unpack_contractions=True,\n",
    "            spell_correct_elong=True,\n",
    "\n",
    "            # tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "            tokenizer=RegexpTokenizer(r'\\w+').tokenize,\n",
    "\n",
    "            dicts=[emoticons]\n",
    "        )\n",
    "\n",
    "    return text_processor\n",
    "\n",
    "def remove_stopword(tokens):\n",
    "    stop_words = stopwords.words('english')\n",
    "#     stop_words.append('url')\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def stemming(tokens, ps):\n",
    "    tokens = [ps.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def lemmatizer(tokens, wn):\n",
    "    tokens = [wn.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def remove_last_url(tokens):\n",
    "    if len(tokens) > 0 and tokens[-1] == 'url':\n",
    "        return tokens[:-1]\n",
    "    else:\n",
    "        return tokens\n",
    "    \n",
    "def pre_process(s):\n",
    "    text = s.content\n",
    "    text = text.replace(\"\\/\", '/')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = text_processor.pre_process_doc(text)\n",
    "    tokens = remove_stopword(tokens)\n",
    "    tokens = stemming(tokens, ps)\n",
    "    tokens = lemmatizer(tokens, wn)\n",
    "    # tokens = remove_last_url(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "text_processor = create_text_processor()\n",
    "ps = nltk.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def extract_content_features(s):\n",
    "    text = s.content\n",
    "    text_len = len(text)\n",
    "    capital_ratio = sum(1 for c in text if c.isupper()) / text_len\n",
    "    question_marks = 1 if '?' in text else 0\n",
    "    exclamation_marks = 1 if '!' in text else 0\n",
    "    period_marks = 1 if '.' in text else 0\n",
    "#     word_count = len(text.split())\n",
    "    \n",
    "    return torch.tensor([capital_ratio, question_marks, exclamation_marks, period_marks])\n",
    "\n",
    "def load_tweet_content(tweet_content_file):\n",
    "    def embed_content(s):\n",
    "#         tokens = s.content_tokens\n",
    "#         content_embedding = np.array([word_vectors[token] for token in tokens if token in word_vectors])\n",
    "#         content_embedding = np.mean(content_embedding, axis=0)\n",
    "#         if np.isnan(content_embedding).any():\n",
    "#             content_embedding = np.zeros((embed_dim, ))\n",
    "#         return content_embedding\n",
    "    \n",
    "        tokens = s.content_tokens\n",
    "        content_embedding = torch.tensor([word_vectors[token] for token in tokens if token in word_vectors], dtype=torch.float)\n",
    "        content_embedding = torch.mean(content_embedding, axis=0)\n",
    "        if torch.isnan(content_embedding).any():\n",
    "            content_embedding = torch.zeros((embed_dim, ))\n",
    "        return content_embedding\n",
    "\n",
    "    content_df = pd.read_csv(tweet_content_file, sep='\\t', header=None, names=['id', 'content'])\n",
    "    content_df['content_features'] = content_df.apply(extract_content_features, axis=1)\n",
    "    content_df['word_count'] = content_df['content'].apply(lambda x: len(x.split()))\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    content_df['word_count'] = min_max_scaler.fit_transform(content_df[['word_count']].values.astype(float))\n",
    "    \n",
    "    content_df['content_tokens'] = content_df.apply(pre_process, axis=1)\n",
    "    content_df['content_embedding'] = content_df.apply(embed_content, axis=1)\n",
    "    content_dict = {row['id']:torch.cat((row['content_embedding'], row['content_features'], torch.tensor([row['word_count']]))) for i, row in content_df.iterrows()}\n",
    "    \n",
    "    return content_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_user_info(user_info_file):\n",
    "    user_info_dict = {}\n",
    "    user_df = pd.read_csv(user_info_file, sep='\\t')\n",
    "    user_df = user_df.drop('has_description', 1)\n",
    "    user_df = user_df.drop('has_url', 1)\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    user_df['account_age'] = min_max_scaler.fit_transform(user_df[['account_age']].values.astype(float))\n",
    "    user_df['status_count'] = min_max_scaler.fit_transform(user_df[['status_count']].values.astype(float))\n",
    "    user_df['follower_count'] = min_max_scaler.fit_transform(user_df[['follower_count']].values.astype(float))\n",
    "    user_df['friend_count'] = min_max_scaler.fit_transform(user_df[['friend_count']].values.astype(float))\n",
    "    \n",
    "    user_info_dict = {row['uid']: torch.tensor(row.values[1:], dtype=torch.float32) for i, row in user_df.iterrows()}\n",
    "    return user_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rumor_graphs(tree_dir_path, avai_ids):\n",
    "    graphs = {}\n",
    "    for f in os.listdir(tree_dir_path):\n",
    "        file_path = os.path.join(tree_dir_path, f)\n",
    "\n",
    "        if os.path.isfile(file_path) and '.txt' in file_path:\n",
    "            G = nx.DiGraph()\n",
    "            root_id = int(f.split('.')[0])\n",
    "            if root_id not in avai_ids:\n",
    "                continue\n",
    "\n",
    "            tweet_level = {}\n",
    "            tweet_ids = []\n",
    "            G.add_node(root_id)\n",
    "            tweet_level[root_id] = 0\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    line_arr = line.split(\"'\")\n",
    "                    if 'ROOT' in line:\n",
    "                        user2 = int(line_arr[7])\n",
    "                        tweet2 = int(line_arr[9])\n",
    "                        tweet2_time = float(line_arr[11])\n",
    "                        \n",
    "                        G.add_node(tweet2, user=user2, time=tweet2_time)\n",
    "                        \n",
    "                    elif 'ROOT' not in line:\n",
    "                        user1 = int(line_arr[1])\n",
    "                        tweet1 = int(line_arr[3])\n",
    "                        tweet1_time = float(line_arr[5])\n",
    "                        user2 = int(line_arr[7])\n",
    "                        tweet2 = int(line_arr[9])\n",
    "                        tweet2_time = float(line_arr[11])\n",
    "                        \n",
    "#                         if tweet1 in avai_ids and tweet2 in avai_ids:\n",
    "                        if tweet1 not in tweet_level:\n",
    "                            tweet_level[tweet1] = 0\n",
    "                        if tweet2 not in tweet_level:\n",
    "                            tweet_level[tweet2] = tweet_level[tweet1] + 1\n",
    "\n",
    "                        if tweet1 != tweet2:\n",
    "                            G.add_node(tweet1, user=user1, time=tweet1_time)\n",
    "                            G.add_node(tweet2, user=user2, time=tweet2_time)\n",
    "                            G.add_edge(tweet2, tweet1)\n",
    "                \n",
    "#                 tweet_ids.sort(key=itemgetter(1))\n",
    "#                 tweet_ids = [tweet_id for (tweet_id, _) in tweet_ids]\n",
    "                tweet_ids = [x for x, _ in sorted(tweet_level.items(), key=lambda kv: kv[1])]\n",
    "                graphs[root_id] = (tweet_ids, G)\n",
    "        \n",
    "    return graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_file):\n",
    "    label_df = pd.read_csv(label_file, sep=':', header=None, names=['label', 'id'])\n",
    "    if args['dataset'] == 'Twitter':\n",
    "        label_df['label'] = label_df['label'].map({'unverified': 0, 'non-rumor': 1, 'true': 2, 'false': 3})\n",
    "    elif args['dataset'] == 'PHEME':\n",
    "        label_df['label'] = label_df['label'].map({'rumor': 0, 'non-rumor': 1})\n",
    "    label_dict = {row['id']:row['label'] for i, row in label_df.iterrows()}\n",
    "    \n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dict = load_tweet_content(os.path.join(args['data_dir'], args['tweet_content_file']))\n",
    "user_info_dict = load_user_info(os.path.join(args['data_dir'], args['user_info_file']))\n",
    "graphs = load_rumor_graphs(os.path.join(args['data_dir'], args['tree_dir']), avai_ids=content_dict.keys())\n",
    "label_dict = load_labels(os.path.join(args['data_dir'], args['label_file']))\n",
    "\n",
    "extra_content_dim = 5\n",
    "user_dim = 5\n",
    "time_dim = 1\n",
    "input_dim = embed_dim + extra_content_dim + user_dim + time_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 0\n",
    "for root_id, (tweet_ids, G) in graphs.items():\n",
    "    tweet_ids = tweet_ids[:args['max_graph_size']]\n",
    "    for tweet_id in tweet_ids:\n",
    "        tweet_time = G.nodes[tweet_id]['time']\n",
    "        if max_time < tweet_time:\n",
    "            max_time = tweet_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_features = np.zeros((len(graphs), input_dim))\n",
    "label = np.zeros((len(graphs), ))\n",
    "\n",
    "for i, (root_id, (tweet_ids, G)) in enumerate(graphs.items()):\n",
    "    pageranks = nx.pagerank(G, alpha=0.8)\n",
    "    \n",
    "    graph_vector = np.zeros((input_dim))\n",
    "    \n",
    "#     Only use source tweet\n",
    "#     content_feature = content_dict[root_id] if root_id in content_dict else torch.zeros((embed_dim + extra_content_dim, ))\n",
    "#     user_id = G.nodes[root_id]['user']\n",
    "#     social_feature = user_info_dict[user_id] if user_id in user_info_dict else torch.zeros((user_dim))\n",
    "#     time_feature = torch.tensor([G.nodes[root_id]['time']/max_time])\n",
    "#     graph_vector = torch.cat((content_feature, social_feature, time_feature)).numpy()\n",
    "\n",
    "    for tweet_id, rank in pageranks.items():\n",
    "        content_feature = content_dict[tweet_id] if tweet_id in content_dict else torch.zeros((embed_dim + extra_content_dim, ))\n",
    "        user_id = G.nodes[tweet_id]['user']\n",
    "        social_feature = user_info_dict[user_id] if user_id in user_info_dict else torch.zeros((user_dim))\n",
    "        time_feature = torch.tensor([G.nodes[tweet_id]['time']/max_time])\n",
    "        node_vector = torch.cat((content_feature, social_feature, time_feature))\n",
    "        \n",
    "        graph_vector += node_vector.numpy() * rank\n",
    "    \n",
    "    graph_features[i] = graph_vector\n",
    "    label[i] = label_dict[root_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unverified 0.46462694047084485\n",
      "Non 0.733820055563141\n",
      "True 0.7261458171574096\n",
      "False 0.6294542666947303\n",
      "Macro F1 0.6385117699715314\n",
      "Acc 0.6649289204233859\n"
     ]
    }
   ],
   "source": [
    "splits = list(StratifiedKFold(n_splits=args['n_splits'], shuffle=True, random_state=args['seed']).split(graph_features, label))\n",
    "acc_list = []\n",
    "unverified_list = []\n",
    "non_list = []\n",
    "true_list = []\n",
    "false_list = []\n",
    "macro_list = []\n",
    "\n",
    "for idx, (train_idx, val_idx) in enumerate(splits):\n",
    "    X_train = graph_features[train_idx]\n",
    "    X_test = graph_features[val_idx]\n",
    "    y_train = label[train_idx]\n",
    "    y_test = label[val_idx]\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=200, max_depth=6, n_jobs=8, random_state=args['seed'])\n",
    "#     clf = SVC()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    acc_list.append(acc)\n",
    "    \n",
    "    if args['dataset'] == 'Twitter':\n",
    "        f1 = f1_score(y_test, y_pred, labels=[0, 1, 2, 3], average=None)\n",
    "        macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "        unverified_list.append(f1[0])\n",
    "        non_list.append(f1[1])\n",
    "        true_list.append(f1[2])\n",
    "        false_list.append(f1[3])\n",
    "        macro_list.append(macro_f1)\n",
    "    elif args['dataset'] == 'PHEME':\n",
    "        f1 = f1_score(y_test, y_pred, pos_label=0)\n",
    "        precision = precision_score(y_test, y_pred, pos_label=0)\n",
    "        recall = recall_score(y_test, y_pred, pos_label=0)\n",
    "\n",
    "        true_list.append(precision)\n",
    "        non_list.append(recall)\n",
    "        macro_list.append(f1)\n",
    "    \n",
    "print('Unverified', np.mean(np.array(unverified_list)))\n",
    "print('Non', np.mean(np.array(non_list)))\n",
    "print('True', np.mean(np.array(true_list)))\n",
    "print('False', np.mean(np.array(false_list)))\n",
    "print('Macro F1', np.mean(np.array(macro_list)))\n",
    "\n",
    "print('Acc', np.mean(np.array(acc_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unverified 0.5460368654645429\n",
    "# Non 0.6849373410122241\n",
    "# True 0.7702281783475089\n",
    "# False 0.6253053197891162\n",
    "# Macro F1 0.6566269261533481\n",
    "# Acc 0.6715294641186025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(graph_features, label, test_size=0.3, random_state=args['seed'])\n",
    "# clf = RandomForestClassifier(n_estimators=200, n_jobs=8, random_state=0)\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "# accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unverified 0.6436432034878614\n",
    "# Non 0.711834696815937\n",
    "# True 0.7954398153498852\n",
    "# False 0.6949399324737426\n",
    "# Macro F1 0.7114644120318566\n",
    "# Acc 0.7098709624195918\n",
    "\n",
    "# Unverified 0.5885150162718416\n",
    "# Non 0.6810329222431714\n",
    "# True 0.7913320557485826\n",
    "# False 0.6376347697709926\n",
    "# Macro F1 0.6746286910086471\n",
    "# Acc 0.6855363387287011\n",
    "\n",
    "# Twitter15+Twitter16\n",
    "# Pagerank\n",
    "# Unverified 0.6184844667488563\n",
    "# Non 0.6877254725833382\n",
    "# True 0.8014874870187265\n",
    "# False 0.6757390907424241\n",
    "# Macro F1 0.6958591292733363\n",
    "# Acc 0.7044000490634406\n",
    "\n",
    "# Average\n",
    "# Unverified 0.46462694047084485\n",
    "# Non 0.733820055563141\n",
    "# True 0.7261458171574096\n",
    "# False 0.6294542666947303\n",
    "# Macro F1 0.6385117699715314\n",
    "# Acc 0.6649289204233859\n",
    "\n",
    "# Only source post\n",
    "# Unverified 0.6097501701076816\n",
    "# Non 0.7750018202823165\n",
    "# True 0.7915742768527358\n",
    "# False 0.6878029802804813\n",
    "# Macro F1 0.7160323118808037\n",
    "# Acc 0.7298821487609796\n",
    "\n",
    "# Source content embedding\n",
    "# Unverified 0.5723346120196513\n",
    "# Non 0.6842519991187694\n",
    "# True 0.8007586753312985\n",
    "# False 0.6431618353894495\n",
    "# Macro F1 0.6751267804647922\n",
    "# Acc 0.6875330608313014\n",
    "\n",
    "# PHEME\n",
    "# Pagerank\n",
    "# Unverified nan\n",
    "# Non 0.6985932085932086\n",
    "# True 0.7886166154804164\n",
    "# False nan\n",
    "# Macro F1 0.7408175379459995\n",
    "# Acc 0.8172784779713277\n",
    "\n",
    "# Average\n",
    "# Unverified nan\n",
    "# Non 0.6373943173943173\n",
    "# True 0.7704774794451561\n",
    "# False nan\n",
    "# Macro F1 0.6976272743850336\n",
    "# Acc 0.7934665689619967\n",
    "\n",
    "# Only source post\n",
    "# Unverified nan\n",
    "# Non 0.7123319473319473\n",
    "# True 0.7999817221245831\n",
    "# False nan\n",
    "# Macro F1 0.7535466808589432\n",
    "# Acc 0.8258383089136696"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
