{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "from treelib import Node, Tree\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'dataset': 'PHEME', #Twitter/PHEME\n",
    "#     'data_dir': '../rumor_detection_acl2017/twitter_all/', #Twitter\n",
    "    'data_dir': '../pheme_twitter/', #PHEME\n",
    "    'tweet_content_file': 'tweet_contents_merge_fix.txt',\n",
    "    'tree_dir': 'tree',\n",
    "    'label_file': 'label.txt',\n",
    "    'user_info_file': 'user_info.txt',\n",
    "    'w2v': 'twitter_preprocess_4_w2c_400.txt',\n",
    "\n",
    "    'max_graph_size': 50,\n",
    "    'K': 2,\n",
    "    'hidden_dim': 50,\n",
    "    'target_size': 2, # 4/2\n",
    "    'batch_size': 1,\n",
    "    'learning_rate': 2e-3,\n",
    "    'n_epoches': 40, # 70/40\n",
    "    'logging_steps': 100,\n",
    "    'do_eval': True,\n",
    "    'n_splits': 5,\n",
    "    'seed': 1234,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_everything(args['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_processor():\n",
    "    text_processor = TextPreProcessor(\n",
    "            normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "                       'time', 'date', 'number'],\n",
    "            fix_html=True,\n",
    "            segmenter=\"twitter\",\n",
    "            corrector=\"twitter\",\n",
    "\n",
    "            unpack_hashtags=True,\n",
    "            unpack_contractions=True,\n",
    "            spell_correct_elong=True,\n",
    "\n",
    "            # tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "            tokenizer=RegexpTokenizer(r'\\w+').tokenize,\n",
    "\n",
    "            dicts=[emoticons]\n",
    "        )\n",
    "\n",
    "    return text_processor\n",
    "\n",
    "def remove_stopword(tokens):\n",
    "    stop_words = stopwords.words('english')\n",
    "#     stop_words.append('url')\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def stemming(tokens, ps):\n",
    "    tokens = [ps.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def lemmatizer(tokens, wn):\n",
    "    tokens = [wn.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def remove_last_url(tokens):\n",
    "    if len(tokens) > 0 and tokens[-1] == 'url':\n",
    "        return tokens[:-1]\n",
    "    else:\n",
    "        return tokens\n",
    "    \n",
    "def pre_process(s):\n",
    "    text = s.content\n",
    "    text = text.replace(\"\\/\", '/')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = text_processor.pre_process_doc(text)\n",
    "    tokens = remove_stopword(tokens)\n",
    "    tokens = stemming(tokens, ps)\n",
    "    tokens = lemmatizer(tokens, wn)\n",
    "    # tokens = remove_last_url(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format(args['data_dir'] + args['w2v'], binary=False)\n",
    "embed_dim = word_vectors.vector_size\n",
    "text_processor = create_text_processor()\n",
    "ps = nltk.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def extract_content_features(s):\n",
    "    text = s.content\n",
    "    text_len = len(text)\n",
    "    capital_ratio = sum(1 for c in text if c.isupper()) / text_len\n",
    "    question_marks = 1 if '?' in text else 0\n",
    "    exclamation_marks = 1 if '!' in text else 0\n",
    "    period_marks = 1 if '.' in text else 0\n",
    "#     word_count = len(text.split())\n",
    "    \n",
    "    return torch.tensor([capital_ratio, question_marks, exclamation_marks, period_marks])\n",
    "\n",
    "def load_tweet_content(tweet_content_file):\n",
    "    def embed_content(s):\n",
    "        tokens = s.content_tokens\n",
    "        content_embedding = torch.tensor([word_vectors[token] for token in tokens if token in word_vectors], dtype=torch.float)\n",
    "        content_embedding = torch.mean(content_embedding, axis=0)\n",
    "        if torch.isnan(content_embedding).any():\n",
    "            content_embedding = torch.zeros((embed_dim, ))\n",
    "        return content_embedding\n",
    "\n",
    "    content_df = pd.read_csv(tweet_content_file, sep='\\t', header=None, names=['id', 'content'], lineterminator='\\n')\n",
    "    content_df['content_features'] = content_df.apply(extract_content_features, axis=1)\n",
    "    content_df['word_count'] = content_df['content'].apply(lambda x: len(x.split()))\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    content_df['word_count'] = min_max_scaler.fit_transform(content_df[['word_count']].values.astype(float))\n",
    "    \n",
    "    content_df['content_tokens'] = content_df.apply(pre_process, axis=1)\n",
    "    content_df['content_embedding'] = content_df.apply(embed_content, axis=1)\n",
    "    content_dict = {row['id']:torch.cat((row['content_embedding'], row['content_features'], torch.tensor([row['word_count']]))) for i, row in content_df.iterrows()}\n",
    "    \n",
    "    return content_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_user_info(user_info_file):\n",
    "    user_info_dict = {}\n",
    "    user_df = pd.read_csv(user_info_file, sep='\\t')\n",
    "    user_df = user_df.drop('has_description', 1)\n",
    "    user_df = user_df.drop('has_url', 1)\n",
    "\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    user_df['account_age'] = min_max_scaler.fit_transform(user_df[['account_age']].values.astype(float))\n",
    "    user_df['status_count'] = min_max_scaler.fit_transform(user_df[['status_count']].values.astype(float))\n",
    "    user_df['follower_count'] = min_max_scaler.fit_transform(user_df[['follower_count']].values.astype(float))\n",
    "    user_df['friend_count'] = min_max_scaler.fit_transform(user_df[['friend_count']].values.astype(float))\n",
    "    \n",
    "    user_info_dict = {row['uid']: torch.tensor(row.values[1:], dtype=torch.float32) for i, row in user_df.iterrows()}\n",
    "    return user_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rumor_trees(tree_dir_path, avai_ids):\n",
    "    trees = {}\n",
    "    for f in os.listdir(tree_dir_path):\n",
    "        file_path = os.path.join(tree_dir_path, f)\n",
    "\n",
    "        if os.path.isfile(file_path) and '.txt' in file_path:\n",
    "            tree = Tree()\n",
    "            tweet_ids = []\n",
    "            root_id = int(f.split('.')[0])\n",
    "            if root_id not in avai_ids:\n",
    "                continue\n",
    "            tweet_ids.append(root_id)\n",
    "                \n",
    "            tree.create_node(tag=root_id, identifier=root_id)\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    line_arr = line.split(\"'\")\n",
    "                    if 'ROOT' in line:\n",
    "                        user2 = int(line_arr[7])\n",
    "                        tweet2 = int(line_arr[9])\n",
    "                        tweet2_time = float(line_arr[11])\n",
    "                        \n",
    "                        tree.get_node(tweet2).data=(user2, 0.0)\n",
    "                    elif 'ROOT' not in line:\n",
    "                        user1 = int(line_arr[1])\n",
    "                        tweet1 = int(line_arr[3])\n",
    "                        tweet1_time = float(line_arr[5])\n",
    "                        user2 = int(line_arr[7])\n",
    "                        tweet2 = int(line_arr[9])\n",
    "                        tweet2_time = float(line_arr[11])\n",
    "                        \n",
    "                        if tweet2 not in tweet_ids and tweet1 in tweet_ids: \n",
    "                            tweet_ids.append(tweet2)\n",
    "                        \n",
    "                        if tweet2 not in tree.nodes and tweet1 in tree.nodes:\n",
    "                            tree.create_node(tag=tweet2, identifier=tweet2, parent=tweet1, data=(user2, tweet2_time))\n",
    "                            \n",
    "#                 tweet_ids.reverse()\n",
    "                trees[root_id] = (tweet_ids, tree)\n",
    "        \n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_file):\n",
    "    label_df = pd.read_csv(label_file, sep=':', header=None, names=['label', 'id'])\n",
    "    if args['dataset'] == 'Twitter':\n",
    "        label_df['label'] = label_df['label'].map({'unverified': 0, 'non-rumor': 1, 'true': 2, 'false': 3})\n",
    "    elif args['dataset'] == 'PHEME':\n",
    "        label_df['label'] = label_df['label'].map({'rumor': 0, 'non-rumor': 1})\n",
    "    label_dict = {row['id']:row['label'] for i, row in label_df.iterrows()}\n",
    "    \n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RumorDataset(Dataset):\n",
    "    def __init__(self, adj_matrix_list, tweet_feature_list, label_list):\n",
    "        super(RumorDataset, self).__init__()\n",
    "        self.adj_matrix_list = adj_matrix_list\n",
    "        self.tweet_feature_list = tweet_feature_list\n",
    "        self.label_list = torch.tensor(label_list, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return (self.adj_matrix_list[item], self.tweet_feature_list[item], self.label_list[item])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RumorModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, target_size):\n",
    "        super(RumorModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gru = nn.GRUCell(input_dim, hidden_dim, bias=False)\n",
    "        self.linear_out = nn.Linear(hidden_dim, target_size)\n",
    "        \n",
    "    def forward(self, adj, feature):\n",
    "        hidden_feature = torch.zeros((adj.shape[0], adj.shape[1], self.hidden_dim), device='cuda', dtype=torch.float)\n",
    "        for i in range(adj.shape[1]):\n",
    "            node_adj = adj[:, i, :]\n",
    "            node_hidden_input = torch.bmm(node_adj.unsqueeze(1), hidden_feature)\n",
    "            node_hidden = self.gru(feature[:, i, :], node_hidden_input.squeeze(1))\n",
    "            hidden_feature[:, i, :] = node_hidden\n",
    "            \n",
    "        output = self.linear_out(node_hidden)\n",
    "#         output = F.softmax(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dict = load_tweet_content(os.path.join(args['data_dir'], args['tweet_content_file']))\n",
    "user_info_dict = load_user_info(os.path.join(args['data_dir'], args['user_info_file']))\n",
    "trees = load_rumor_trees(os.path.join(args['data_dir'], args['tree_dir']), avai_ids=content_dict.keys())\n",
    "label_dict = load_labels(os.path.join(args['data_dir'], args['label_file']))\n",
    "\n",
    "extra_content_dim = 5\n",
    "user_dim = 5\n",
    "time_dim = 1\n",
    "input_dim = embed_dim + extra_content_dim + user_dim + time_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 0\n",
    "for root_id, (tweet_ids, tree) in trees.items():\n",
    "    tweet_ids = tweet_ids[:args['max_graph_size']]\n",
    "#     print('root', tree.root)\n",
    "    for tweet in tweet_ids:\n",
    "        if tweet in tree.nodes.keys():\n",
    "#             print(tree.get_node(tweet).data[1])\n",
    "            tweet_time = tree.get_node(tweet).data[1]\n",
    "            if max_time < tweet_time:\n",
    "                max_time = tweet_time\n",
    "        else:\n",
    "            print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix_list = []\n",
    "tweet_feature_list = []\n",
    "label_list = []\n",
    "\n",
    "for root, (tweet_ids, tree) in trees.items():\n",
    "    tweet_ids = tweet_ids[:args['max_graph_size']]\n",
    "    tweet_ids.reverse()\n",
    "    \n",
    "    tweet_feature = torch.zeros((len(tweet_ids), input_dim))\n",
    "    adj_matrix = torch.zeros((len(tweet_ids), len(tweet_ids)))\n",
    "    \n",
    "    for i, tweet in enumerate(tweet_ids):\n",
    "#         tweet_feature[i] = content_dict[tweet] if tweet in content_dict else torch.zeros((embed_dim, ))\n",
    "        \n",
    "        content_feature = content_dict[tweet] if tweet in content_dict else torch.zeros((embed_dim + extra_content_dim, ))\n",
    "        user_id = tree.get_node(tweet).data[0]\n",
    "        social_feature = user_info_dict[user_id] if user_id in user_info_dict else torch.zeros((user_dim))\n",
    "        time_feature = torch.tensor([tree.get_node(tweet).data[1]/max_time])\n",
    "        tweet_feature[i] = torch.cat((content_feature, social_feature, time_feature))\n",
    "        \n",
    "        for child in tree.children(tweet):\n",
    "            if child.identifier in tweet_ids:\n",
    "                j = tweet_ids.index(child.identifier)\n",
    "                adj_matrix[i, j] = 1\n",
    "    \n",
    "    tweet_feature_list.append(tweet_feature)\n",
    "    adj_matrix_list.append(adj_matrix)\n",
    "    label_list.append(label_dict[root])\n",
    "    \n",
    "# tweet_feature_list = torch.tensor(tweet_feature_list, dtype=torch.float)\n",
    "# adj_matrix_list = torch.tensor(adj_matrix_list, dtype=torch.long)\n",
    "# label_list = torch.tensor(label_list, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splits = list(StratifiedKFold(n_splits=args['n_splits'], shuffle=True, random_state=args['seed']).split(tweet_feature_list, label_list))\n",
    "fold_train_acc = np.zeros((args['n_splits'], args['n_epoches']))\n",
    "fold_train_loss = np.zeros((args['n_splits'], args['n_epoches']))\n",
    "fold_val_acc = np.zeros((args['n_splits'], args['n_epoches']))\n",
    "fold_val_loss = np.zeros((args['n_splits'], args['n_epoches']))\n",
    "\n",
    "fold_unverified_f1 = np.zeros((args['n_splits'], args['n_epoches']))\n",
    "fold_non_f1 = np.zeros((args['n_splits'], args['n_epoches']))\n",
    "fold_true_f1 = np.zeros((args['n_splits'], args['n_epoches']))\n",
    "fold_false_f1 = np.zeros((args['n_splits'], args['n_epoches']))\n",
    "fold_macro_f1 = np.zeros((args['n_splits'], args['n_epoches']))\n",
    "\n",
    "for idx, (train_idx, val_idx) in enumerate(splits):\n",
    "    print('Train Fold {}'.format(idx))\n",
    "    \n",
    "    train_dataset = RumorDataset([adj_matrix_list[i] for i in train_idx], \n",
    "                                 [tweet_feature_list[i] for i in train_idx], \n",
    "                                 [label_list[i] for i in train_idx])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True, num_workers=8)\n",
    "\n",
    "    valid_dataset = RumorDataset([adj_matrix_list[i] for i in val_idx], \n",
    "                                 [tweet_feature_list[i] for i in val_idx], \n",
    "                                 [label_list[i] for i in val_idx])\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=args['batch_size'], shuffle=False, num_workers=8)\n",
    "    \n",
    "    model = RumorModel(input_dim=input_dim, hidden_dim=args['hidden_dim'], \n",
    "                       target_size=args['target_size']).cuda()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=args['learning_rate'])\n",
    "#     optimizer = torch.optim.Adagrad(params=model.parameters(), lr=args['learning_rate'])\n",
    "\n",
    "    model.zero_grad()\n",
    "    for epoch in trange(args['n_epoches'], desc='Epoch'):\n",
    "        model.train()\n",
    "        tr_loss = 0.0\n",
    "        \n",
    "        pred_labels = None\n",
    "        true_labels = np.array([])\n",
    "        for index, (adj, feature, label) in enumerate(tqdm_notebook(train_loader, desc='Batch')):\n",
    "            adj = adj.cuda()\n",
    "            feature = feature.cuda()\n",
    "            label = label.cuda()\n",
    "            \n",
    "            preds = model(adj, feature)\n",
    "#             new_label = torch.zeros((preds.shape)).cuda()\n",
    "#             new_label[0, label[0]] = 1.0\n",
    "            loss = criterion(preds, label)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            \n",
    "            true_labels = np.append(true_labels, label.cpu().numpy(), axis=0)\n",
    "            if pred_labels is None:\n",
    "                pred_labels = preds.detach().cpu().numpy()\n",
    "            else:\n",
    "                pred_labels = np.append(pred_labels, preds.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        pred_labels = np.argmax(pred_labels, axis=1)\n",
    "        train_acc = accuracy_score(true_labels, pred_labels)\n",
    "        train_loss = tr_loss / len(train_loader)\n",
    "        fold_train_acc[idx, epoch] = train_acc\n",
    "        fold_train_loss[idx, epoch] = train_loss\n",
    "        print(f\"Epoch {epoch}, train loss {train_loss}, train accuracy {train_acc}\")\n",
    "        \n",
    "        model.eval()\n",
    "        vl_loss = 0.0\n",
    "        pred_labels = None\n",
    "        true_labels = np.array([])\n",
    "        for index, (adj, feature, label) in enumerate(tqdm_notebook(valid_loader, desc='Valid Batch')):\n",
    "            adj = adj.cuda()\n",
    "            feature = feature.cuda()\n",
    "            label = label.cuda()\n",
    "\n",
    "            preds = model(adj, feature)\n",
    "#             new_label = torch.zeros((preds.shape)).cuda()\n",
    "#             new_label[0, label[0]] = 1.0\n",
    "            loss = criterion(preds, label)\n",
    "\n",
    "            vl_loss += loss.item()\n",
    "            true_labels = np.append(true_labels, label.cpu().numpy(), axis=0)\n",
    "            if pred_labels is None:\n",
    "                pred_labels = preds.detach().cpu().numpy()\n",
    "            else:\n",
    "                pred_labels = np.append(pred_labels, preds.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        pred_labels = np.argmax(pred_labels, axis=1)\n",
    "        val_acc = accuracy_score(true_labels, pred_labels)\n",
    "        val_loss = vl_loss / len(valid_loader)\n",
    "        \n",
    "        if args['dataset'] == 'Twitter':\n",
    "            f1 = f1_score(true_labels, pred_labels, labels=[0, 1, 2, 3], average=None)\n",
    "            macro_f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "            \n",
    "            fold_unverified_f1[idx, epoch] = f1[0]\n",
    "            fold_non_f1[idx, epoch] = f1[1]\n",
    "            fold_true_f1[idx, epoch] = f1[2]\n",
    "            fold_false_f1[idx, epoch] = f1[3]\n",
    "            fold_macro_f1[idx, epoch] = macro_f1\n",
    "        elif args['dataset'] == 'PHEME':\n",
    "            f1 = f1_score(true_labels, pred_labels, pos_label=0)\n",
    "            precision = precision_score(true_labels, pred_labels, pos_label=0)\n",
    "            recall = recall_score(true_labels, pred_labels, pos_label=0)\n",
    "            \n",
    "            fold_true_f1[idx, epoch] = precision\n",
    "            fold_non_f1[idx, epoch] = recall\n",
    "            fold_macro_f1[idx, epoch] = f1\n",
    "        \n",
    "        fold_val_acc[idx, epoch] = val_acc\n",
    "        fold_val_loss[idx, epoch] = val_loss\n",
    "        print(f\"Fold {idx}, Epoch {epoch}, valid loss {val_loss}, valid accuracy {val_acc}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(fold_val_acc.mean(axis=0)))\n",
    "max_fold = np.where(fold_val_acc.mean(axis=0) == max(fold_val_acc.mean(axis=0)))\n",
    "print(max_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('macro f1', fold_macro_f1.mean(axis=0)[max_fold])\n",
    "print('non-rumor', fold_non_f1.mean(axis=0)[max_fold])\n",
    "print('true', fold_true_f1.mean(axis=0)[max_fold])\n",
    "print('false', fold_false_f1.mean(axis=0)[max_fold])\n",
    "print('unverified', fold_unverified_f1.mean(axis=0)[max_fold])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(info):\n",
    "    if info=='acc':\n",
    "        data = [\n",
    "                    go.Scatter(x=np.arange(args['n_epoches']), y=np.array(fold_train_acc.mean(axis=0)), name='Train'),\n",
    "                    go.Scatter(x=np.arange(args['n_epoches']), y=np.array(fold_val_acc.mean(axis=0)), name='Valid'),\n",
    "               ]\n",
    "        layout = {\n",
    "                    \"title\": \"Accuracy\",\n",
    "                 }\n",
    "    else:\n",
    "        data = [\n",
    "                    go.Scatter(x=np.arange(args['n_epoches']), y=np.array(fold_train_loss.mean(axis=0)), name='Train'),\n",
    "                    go.Scatter(x=np.arange(args['n_epoches']), y=np.array(fold_val_loss.mean(axis=0)), name='Valid'),\n",
    "               ]\n",
    "        layout = {\n",
    "                    \"title\": \"Loss\",\n",
    "                 }\n",
    "        \n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "    iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(info='acc')\n",
    "plot_result(info='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
