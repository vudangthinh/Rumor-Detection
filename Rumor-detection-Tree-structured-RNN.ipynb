{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "from treelib import Node, Tree\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'data_dir': '../rumor_detection_acl2017/twitter15/',\n",
    "    'tweet_content_file': 'tweet_contents.txt',\n",
    "    'tree_dir': 'tree',\n",
    "    'label_file': 'label.txt',\n",
    "    'w2v': 'twitter_preprocess_3_w2c_400.txt',\n",
    "\n",
    "    'max_graph_size': 50,\n",
    "    'K': 2,\n",
    "    'hidden_dim': 200,\n",
    "    'target_size': 4,\n",
    "    'batch_size': 8,\n",
    "    'learning_rate': 1e-3,\n",
    "    'n_epoches': 70,\n",
    "    'logging_steps': 100,\n",
    "    'do_eval': True,\n",
    "    'aggregator': 'mean',\n",
    "    'n_splits': 5,\n",
    "    'seed': 1234,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_everything(args['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_processor():\n",
    "    text_processor = TextPreProcessor(\n",
    "            normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "                       'time', 'date', 'number'],\n",
    "            fix_html=True,\n",
    "            segmenter=\"twitter\",\n",
    "            corrector=\"twitter\",\n",
    "\n",
    "            unpack_hashtags=True,\n",
    "            unpack_contractions=True,\n",
    "            spell_correct_elong=True,\n",
    "\n",
    "            # tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "            tokenizer=RegexpTokenizer(r'\\w+').tokenize,\n",
    "\n",
    "            dicts=[emoticons]\n",
    "        )\n",
    "\n",
    "    return text_processor\n",
    "\n",
    "def remove_stopword(tokens):\n",
    "    stop_words = stopwords.words('english')\n",
    "#     stop_words.append('url')\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def stemming(tokens, ps):\n",
    "    tokens = [ps.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def lemmatizer(tokens, wn):\n",
    "    tokens = [wn.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def remove_last_url(tokens):\n",
    "    if len(tokens) > 0 and tokens[-1] == 'url':\n",
    "        return tokens[:-1]\n",
    "    else:\n",
    "        return tokens\n",
    "    \n",
    "def pre_process(s):\n",
    "    text = s.content\n",
    "    text = text.replace(\"\\/\", '/')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = text_processor.pre_process_doc(text)\n",
    "    tokens = remove_stopword(tokens)\n",
    "    tokens = stemming(tokens, ps)\n",
    "    tokens = lemmatizer(tokens, wn)\n",
    "    # tokens = remove_last_url(tokens)\n",
    "    n_grams = set.union(set(ngrams(tokens, 1)), set(ngrams(tokens, 2)))\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format(args['data_dir'] + args['w2v'], binary=False)\n",
    "embed_dim = word_vectors.vector_size\n",
    "text_processor = create_text_processor()\n",
    "ps = nltk.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def load_tweet_content(tweet_content_file):\n",
    "    def embed_content(s):\n",
    "        tokens = s.content_tokens\n",
    "        content_embedding = torch.tensor([word_vectors[token] for token in tokens if token in word_vectors], dtype=torch.float)\n",
    "        content_embedding = torch.mean(content_embedding, axis=0)\n",
    "        if torch.isnan(content_embedding).any():\n",
    "            content_embedding = torch.zeros((embed_dim, ))\n",
    "        return content_embedding\n",
    "\n",
    "    content_df = pd.read_csv(tweet_content_file, sep='\\t', header=None, names=['id', 'content'])\n",
    "    content_df['content_tokens'] = content_df.apply(pre_process, axis=1)\n",
    "    content_df['content_embedding'] = content_df.apply(embed_content, axis=1)\n",
    "    content_dict = {row['id']:row['content_embedding'] for i, row in content_df.iterrows()}\n",
    "    \n",
    "    return content_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rumor_trees(tree_dir_path, content_dict):\n",
    "    trees = {}\n",
    "    for f in os.listdir(tree_dir_path):\n",
    "        file_path = os.path.join(tree_dir_path, f)\n",
    "\n",
    "        if os.path.isfile(file_path) and '.txt' in file_path:\n",
    "            tree = Tree()\n",
    "            tweet_ids = []\n",
    "            root_id = int(f.split('.')[0])\n",
    "            tweet_ids.append(root_id)\n",
    "            if root_id in content_dict:\n",
    "                content = content_dict[root_id]\n",
    "            else:\n",
    "                content = torch.zeros((embed_dim, ))\n",
    "            tree.create_node(tag=root_id, identifier=root_id, data=content)\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    line_arr = line.split(\"'\")\n",
    "                    if 'ROOT' not in line:\n",
    "                        user1 = int(line_arr[1])\n",
    "                        tweet1 = int(line_arr[3])\n",
    "                        user2 = int(line_arr[7])\n",
    "                        tweet2 = int(line_arr[9])\n",
    "                        \n",
    "                        if tweet2 not in tweet_ids: \n",
    "                            tweet_ids.append(tweet2)\n",
    "                        \n",
    "                        if tweet2 not in tree.nodes:\n",
    "                            if tweet2 in content_dict:\n",
    "                                content = content_dict[tweet2]\n",
    "                            else:\n",
    "                                content = torch.zeros((embed_dim, ))\n",
    "                            tree.create_node(tag=tweet2, identifier=tweet2, parent=tweet1, data=content)\n",
    "                \n",
    "                tweet_ids.reverse()\n",
    "                trees[root_id] = (tweet_ids, tree)\n",
    "        \n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_file):\n",
    "    label_df = pd.read_csv(label_file, sep=':', header=None, names=['label', 'id'])\n",
    "    label_df['label'] = label_df['label'].map({'unverified': 0, 'non-rumor': 1, 'true': 2, 'false': 3})\n",
    "    label_dict = {row['id']:row['label'] for i, row in label_df.iterrows()}\n",
    "    \n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RumorDataset(Dataset):\n",
    "    def __init__(self, ids_list, tree_list, label_list):\n",
    "        super(RumorDataset, self).__init__()\n",
    "        self.ids_list = ids_list\n",
    "        self.tree_list = tree_list\n",
    "        self.label_list = label_list\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return (self.ids_list[item], self.tree_list[item], self.label_list[item])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RumorModel(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, target_size):\n",
    "        super(RumorModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.gru = nn.GRUCell(embed_dim, hidden_dim, bias=False)\n",
    "        self.linear_out = nn.Linear(hidden_dim, target_size)\n",
    "        \n",
    "    def forward(self, node_list, tree):\n",
    "        node_out_dict = {}\n",
    "        for node in node_list:\n",
    "            node_input = tree.get_node(node).data\n",
    "            node_hidden = torch.zeros((self.hidden_dim, ))\n",
    "            childrens = tree.children(node)\n",
    "            for child in childrens:\n",
    "                node_hidden += node_out_dict[child.identifier]\n",
    "                \n",
    "            node_out_dict[node] = self.gru(node_input.unsqueeze(0), node_hidden.unsqueeze(0)).squeeze()\n",
    "            \n",
    "        last_node_hidden = node_out_dict[node_list[-1]]\n",
    "        output = self.linear_out(last_node_hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dict = load_tweet_content(os.path.join(args['data_dir'], args['tweet_content_file']))\n",
    "trees = load_rumor_trees(os.path.join(args['data_dir'], args['tree_dir']), content_dict)\n",
    "label_dict = load_labels(os.path.join(args['data_dir'], args['label_file']))\n",
    "ids_list = []\n",
    "tree_list = []\n",
    "label_list = []\n",
    "\n",
    "for root, (tweet_ids, tree) in trees.items():\n",
    "    ids_list.append(tweet_ids)\n",
    "    tree_list.append(tree)\n",
    "    label_list.append(label_dict[root])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = list(StratifiedKFold(n_splits=args['n_splits'], shuffle=True, random_state=args['seed']).split(tree_list, label_list))\n",
    "\n",
    "for idx, (train_idx, val_idx) in enumerate(splits):\n",
    "    print('Train Fold {}'.format(idx))\n",
    "    \n",
    "    train_ids_list = [ids_list[i] for i in train_idx]\n",
    "    train_tree_list = [tree_list[i] for i in train_idx]\n",
    "    train_label_list = [label_list[i] for i in train_idx]\n",
    "    \n",
    "    valid_ids_list = [ids_list[i] for i in val_idx]\n",
    "    valid_tree_list = [tree_list[i] for i in val_idx]\n",
    "    valid_label_list = [label_list[i] for i in val_idx]\n",
    "    \n",
    "    model = RumorModel(embed_dim=embed_dim, hidden_dim=args['hidden_dim'], \n",
    "                       target_size=args['target_size'])\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=args['learning_rate'])\n",
    "\n",
    "    for epoch in trange(args['n_epoches'], desc='Epoch'):\n",
    "        model.train()\n",
    "        tr_loss = 0.0\n",
    "\n",
    "        for tweet_ids, tree, label in tqdm_notebook(zip(train_ids_list, train_tree_list, train_label_list)):\n",
    "            preds = model(tweet_ids, tree)\n",
    "            loss = criterion(preds.unsqueeze(0), torch.tensor(label).unsqueeze(0))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "        train_loss = tr_loss / len(label_list)\n",
    "        print(f\"Epoch {epoch}, train loss {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
