{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from treelib import Node, Tree\n",
    "import networkx as nx\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'data_dir': '../rumor_detection_acl2017/twitter15/',\n",
    "    'tweet_content_file': 'tweet_contents.txt',\n",
    "    'tree_dir': 'tree',\n",
    "    'label_file': 'label.txt',\n",
    "    'n_splits': 5,\n",
    "    'seed': 1234,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_processor():\n",
    "    text_processor = TextPreProcessor(\n",
    "            normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "                       'time', 'date', 'number'],\n",
    "            fix_html=True,\n",
    "            segmenter=\"twitter\",\n",
    "            corrector=\"twitter\",\n",
    "\n",
    "            unpack_hashtags=True,\n",
    "            unpack_contractions=True,\n",
    "            spell_correct_elong=True,\n",
    "\n",
    "            # tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "            tokenizer=RegexpTokenizer(r'\\w+').tokenize,\n",
    "\n",
    "            dicts=[emoticons]\n",
    "        )\n",
    "\n",
    "    return text_processor\n",
    "\n",
    "def remove_stopword(tokens):\n",
    "    stop_words = stopwords.words('english')\n",
    "#     stop_words.append('url')\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "def stemming(tokens, ps):\n",
    "    tokens = [ps.stem(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def lemmatizer(tokens, wn):\n",
    "    tokens = [wn.lemmatize(token) for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def remove_last_url(tokens):\n",
    "    if len(tokens) > 0 and tokens[-1] == 'url':\n",
    "        return tokens[:-1]\n",
    "    else:\n",
    "        return tokens\n",
    "    \n",
    "def pre_process(s):\n",
    "    text = s.content\n",
    "    text = text.replace(\"\\/\", '/')\n",
    "    text = text.lower()\n",
    "\n",
    "    tokens = text_processor.pre_process_doc(text)\n",
    "    tokens = remove_stopword(tokens)\n",
    "    tokens = stemming(tokens, ps)\n",
    "    tokens = lemmatizer(tokens, wn)\n",
    "    # tokens = remove_last_url(tokens)\n",
    "    n_grams = set.union(set(ngrams(tokens, 1)), set(ngrams(tokens, 2)))\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = create_text_processor()\n",
    "ps = nltk.PorterStemmer()\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def load_tweet_content(tweet_content_file):\n",
    "    content_df = pd.read_csv(tweet_content_file, sep='\\t', header=None, names=['id', 'content'])\n",
    "    content_df['n_grams'] = content_df.apply(pre_process, axis=1)\n",
    "    content_dict = {row['id']:row['n_grams'] for i, row in content_df.iterrows()}\n",
    "    \n",
    "    return content_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rumor_trees(tree_dir_path, content_dict):\n",
    "    trees = {}\n",
    "    for f in os.listdir(tree_dir_path):\n",
    "        file_path = os.path.join(tree_dir_path, f)\n",
    "\n",
    "        if os.path.isfile(file_path) and '.txt' in file_path:\n",
    "            tree = Tree()\n",
    "            root_id = int(f.split('.')[0])\n",
    "            if root_id in content_dict:\n",
    "                content = content_dict[root_id]\n",
    "            else:\n",
    "                content = set()\n",
    "            tree.create_node(tag=root_id, identifier=root_id, data=(content, 0.0))\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    line_arr = line.split(\"'\")\n",
    "                    if 'ROOT' not in line:\n",
    "                        user1 = int(line_arr[1])\n",
    "                        tweet1 = int(line_arr[3])\n",
    "                        tweet1_time = float(line_arr[5])\n",
    "                        user2 = int(line_arr[7])\n",
    "                        tweet2 = int(line_arr[9])\n",
    "                        tweet2_time = float(line_arr[11])\n",
    "                        \n",
    "                        if tweet2 not in tree.nodes:\n",
    "                            if tweet2 in content_dict:\n",
    "                                content = content_dict[tweet2]\n",
    "                            else:\n",
    "                                content = set()\n",
    "                            tree.create_node(tag=tweet2, identifier=tweet2, parent=tweet1, data=(content, tweet2_time))\n",
    "                \n",
    "                trees[root_id] = tree\n",
    "        \n",
    "    return trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_file):\n",
    "    label_df = pd.read_csv(label_file, sep=':', header=None, names=['label', 'id'])\n",
    "    label_df['label'] = label_df['label'].map({'unverified': 0, 'non-rumor': 1, 'true': 2, 'false': 3})\n",
    "    label_dict = {row['id']:row['label'] for i, row in label_df.iterrows()}\n",
    "    \n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_similarity(node1, node2, alpha=0):\n",
    "    c1, t1 = node1.data\n",
    "    c2, t2 = node2.data\n",
    "\n",
    "    t = abs(t1 - t2)\n",
    "    if len(c1) == 0 or len(c2) == 0:\n",
    "        content_similarity = 0.0\n",
    "    else:\n",
    "        content_similarity = len(set.intersection(c1, c2)) / len(set.union(c1, c2))\n",
    "        \n",
    "#     return content_similarity\n",
    "    return np.exp(-t) * content_similarity\n",
    "\n",
    "def most_similarity_nodes(T1, T2):\n",
    "    node_pairs = {}\n",
    "\n",
    "    for node1 in T1.all_nodes_itr():\n",
    "        highest_simil = -1\n",
    "        similar_node = Node()\n",
    "        for node2 in T2.all_nodes_itr():\n",
    "            node_simil = node_similarity(node1, node2)\n",
    "            if highest_simil < node_simil:\n",
    "                highest_simil = node_simil\n",
    "                similar_node = node2\n",
    "\n",
    "        node_pairs[node1] = similar_node\n",
    "\n",
    "    return node_pairs\n",
    "\n",
    "def sub_tree_similarity(subtree1, subtree2, subtree_similarity_dict):\n",
    "    root_similar = node_similarity(subtree1.get_node(subtree1.root), subtree2.get_node(subtree2.root))\n",
    "    if subtree1.depth() == 0 or subtree2.depth() == 0:\n",
    "        return root_similar\n",
    "    else:\n",
    "        children1 = subtree1.children(subtree1.root)\n",
    "        children2 = subtree2.children(subtree2.root)\n",
    "\n",
    "        nc_min = min(len(children1), len(children2))\n",
    "\n",
    "        multiplication = 1.0\n",
    "        for i in range(nc_min):\n",
    "            child1 = children1[i]\n",
    "            child2 = children2[i]\n",
    "            if (child1.identifier, child2.identifier) in subtree_similarity_dict:\n",
    "                child_similar = subtree_similarity_dict[(child1.identifier, child2.identifier)]\n",
    "            else:\n",
    "                child_tree1 = subtree1.subtree(child1.identifier)\n",
    "                child_tree2 = subtree2.subtree(child2.identifier)\n",
    "\n",
    "                child_similar = sub_tree_similarity(child_tree1, child_tree2, subtree_similarity_dict)\n",
    "\n",
    "                subtree_similarity_dict[(child1.identifier, child2.identifier)] = child_similar\n",
    "            multiplication *= (1 + child_similar)\n",
    "\n",
    "        return root_similar * multiplication\n",
    "\n",
    "def tree_similarity(T1, T2):\n",
    "    subtree_similarity_dict = {}\n",
    "    node_pairs_1 = most_similarity_nodes(T1, T2)\n",
    "    tree_simil_1 = 0.0\n",
    "    for node1, node2 in node_pairs_1.items():\n",
    "        subtree1 = T1.subtree(node1.identifier)\n",
    "        subtree2 = T2.subtree(node2.identifier)\n",
    "        tree_simil_1 += sub_tree_similarity(subtree1, subtree2, subtree_similarity_dict)\n",
    "\n",
    "    node_pairs_2 = most_similarity_nodes(T2, T1)\n",
    "    tree_simil_2 = 0.0\n",
    "    for node2, node1 in node_pairs_2.items():\n",
    "        subtree1 = T1.subtree(node1.identifier)\n",
    "        subtree2 = T2.subtree(node2.identifier)\n",
    "        tree_simil_2 += sub_tree_similarity(subtree1, subtree2, subtree_similarity_dict)\n",
    "\n",
    "    return tree_simil_1 + tree_simil_2\n",
    "\n",
    "def similarity_matrix(tree_list1, tree_list2):\n",
    "    similar_matrix = np.zeros((len(tree_list1), len(tree_list2)))\n",
    "\n",
    "    for i, tree1 in enumerate(tree_list1):\n",
    "        print(f'tree {i}')\n",
    "        for j, tree2 in enumerate(tree_list2):\n",
    "            similar_matrix[i, j] = tree_similarity(tree1, tree2)\n",
    "\n",
    "    return similar_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dict = load_tweet_content(os.path.join(args['data_dir'], args['tweet_content_file']))\n",
    "trees = load_rumor_trees(os.path.join(args['data_dir'], args['tree_dir']), content_dict)\n",
    "label_dict = load_labels(os.path.join(args['data_dir'], args['label_file']))\n",
    "tree_list = []\n",
    "label_list = []\n",
    "\n",
    "for root, tree in trees.items():\n",
    "    tree_list.append(tree)\n",
    "    label_list.append(label_dict[root])\n",
    "    \n",
    "label_list = np.array(label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract kernel matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(args['data_dir'], 'label_5.npy'), label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_matrix = similarity_matrix(tree_list, tree_list)\n",
    "np.save(os.path.join(args['data_dir'], 'kernel_matrix_5.npy'), kernel_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_matrix = np.load(os.path.join(args['data_dir'], 'kernel_matrix_4.npy'))\n",
    "# label_list = np.load(os.path.join(args['data_dir'], 'label.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "kernel_matrix = scaler.fit_transform(kernel_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = list(StratifiedKFold(n_splits=args['n_splits'], shuffle=True, random_state=args['seed']).split(tree_list, label_list))\n",
    "for idx, (train_idx, val_idx) in enumerate(splits):\n",
    "    predict_df = pd.DataFrame()\n",
    "    for label in range(4):\n",
    "        clf = svm.SVC(kernel='precomputed', probability=True)\n",
    "#         clf = RandomForestClassifier(n_estimators=100)\n",
    "        kernel_train = kernel_matrix[train_idx][:, train_idx]\n",
    "        y_train = label_list[train_idx]\n",
    "        y_train[y_train != label] = 4\n",
    "        clf.fit(kernel_train, y_train)\n",
    "\n",
    "        train_pred = clf.predict(kernel_train)\n",
    "        print(f'Fold {idx}, train accuracy {accuracy_score(y_train, train_pred)}')\n",
    "\n",
    "        kernel_test = kernel_matrix[val_idx][:, train_idx]\n",
    "        y_pred = clf.predict_proba(kernel_test)\n",
    "        predict_df[label] = y_pred[:, 0]\n",
    "    \n",
    "    y_test = label_list[val_idx]\n",
    "    y_pred = predict_df.idxmax(axis=1).values\n",
    "    print(f'Fold {idx}, test accuracy {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = np.array(label_list)\n",
    "\n",
    "train_idx, val_idx = train_test_split(range(len(tree_list)), test_size=0.3, random_state=0)\n",
    "clf = svm.SVC(kernel='precomputed')\n",
    "\n",
    "kernel_train = kernel_matrix[train_idx][:, train_idx]\n",
    "y_train = label_list[train_idx]\n",
    "clf.fit(kernel_train, y_train)\n",
    "\n",
    "train_pred = clf.predict(kernel_train)\n",
    "print(f'Train accuracy {accuracy_score(y_train, train_pred)}')\n",
    "\n",
    "kernel_test = kernel_matrix[val_idx][:, train_idx]\n",
    "y_pred = clf.predict(kernel_test)\n",
    "y_test = label_list[val_idx]\n",
    "print(f'Test accuracy {accuracy_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
